{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "texto = \"¿Cuanto tiempo ha pasado desde que iniciamos clase?\"\n",
    "tokenizado = tk.tokenize(texto)\n",
    "print(tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tk = WordPunctTokenizer()\n",
    "texto = \"¿Cuanto tiempo ha pasado desde que iniciamos clase?\"\n",
    "tokenizado = tk.tokenize(texto)\n",
    "print(tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tk = TreebankWordTokenizer()\n",
    "texto = \"¿Cuanto tiempo ha pasado desde que iniciamos clase?\"\n",
    "tokenizado = tk.tokenize(texto)\n",
    "\n",
    "print(tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargue el archivo csv\n",
    "df=pd.read_csv(\"../inputs/inputs.csv\")\n",
    "# aplique los tokenizadores a las oraciones \n",
    "ws_tk = WhitespaceTokenizer()\n",
    "[ ws_tk.tokenize(st) for st in df['oracion']]\n",
    "wp_tk = WordPunctTokenizer()\n",
    "[ wp_tk.tokenize(st) for st in df['oracion']]\n",
    "tb_tk = TreebankWordTokenizer()\n",
    "[ tb_tk.tokenize(st) for st in df['oracion']]\n",
    "\n",
    "\n",
    "[ (ws_tk.tokenize(st), wp_tk.tokenize(st), tb_tk.tokenize(st)) for st in df['oracion']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_file = \"../models/model1.pkl\"\n",
    "m2_file = \"../models/model2.pkl\"\n",
    "m3_file = \"../models/model3.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving WhitespaceTokenizer\n",
    "joblib.dump(ws_tk, m1_file)\n",
    "# saving WordPunctTokenizer\n",
    "joblib.dump(wp_tk, m2_file)\n",
    "# saving TreebackTokenizer\n",
    "joblib.dump(tb_tk, m3_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actividad 2 punto 1\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "sent_tokenization = [wp_tk.tokenize(st) for st in df['oracion']]\n",
    "stemmed = [ [stemmer.stem(a_token) for a_token in a_sentence] for a_sentence in sent_tokenization]\n",
    "\n",
    "print(stemmed)\n",
    "\n",
    "# actividad 2 punto 3\n",
    "joblib.dump(stemmer,\"../models/model4.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actividad 2 punto 2\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "print(df['oracion'])\n",
    "sent_tokenization = [wp_tk.tokenize(st) for st in df['oracion']]\n",
    "lemmatized = [ [lemm.lemmatize(a_token) for a_token in a_sentence] for a_sentence in sent_tokenization]\n",
    "# TODO: preguntar: Si guardo el WordNetLemmatizer, es necesario volver a ejecutar la línea 3 en otra máquina o solo cargo el modelo?\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actividad 2 punto 2\n",
    "import stanza\n",
    "stanza.download(\"es\")\n",
    "process= stanza.Pipeline(lang=\"es\", processors='tokenize,mwt,pos,lemma')\n",
    "docs = [ process(st) for st in df['oracion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= [a_doc.sentences for a_doc in docs]\n",
    "lst_words = [words for a_sentence in sentences for words in a_sentence[0].words]\n",
    "[(a_word.text, a_word.lemma) for a_word  in lst_words]\n",
    "# result=[ [(a_word.text, a_word.lemma)] for sent in doc for a_word in sent]\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download(\"es\")\n",
    "process= stanza.Pipeline(lang=\"es\", processors='tokenize,mwt,pos,lemma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence = \"¿Cuanto tiempo ha pasado desde que iniciamos clase?\"\n",
    "\n",
    "a_doc = process(sentence)\n",
    "sentence= a_doc.sentences \n",
    "lst_words = [words for words in sentence[0].words]\n",
    "\n",
    "[(a_word.text, a_word.lemma) for a_word  in lst_words]\n",
    "# result=[ [(a_word.text, a_word.lemma)] for sent in doc for a_word in sent]\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
